MODEL=ollama/llama3.2
API_BASE=http://ollama:11434

# TinyLLaMA (Para testes locais rápidos e leves )
# Tamanho: ~1,1 bilhão de parâmetros
# Ideal para: Rodar localmente em máquinas com recursos limitados (ex: 4–8 GB de RAM)
# Adequação: Bom para tarefas básicas de análise de texto, como extração de informações simples de currículos, ou triagens iniciais.
# Limitações: Pode ter dificuldade em entender contextos complexos ou nuances em perfis profissionais.

# llama3.2 (Se currículos têm elementos visuais ou vêm de fontes variadas)
# Tamanhos disponíveis:
# Texto: 1B, 3B parâmetros (leves, para dispositivos móveis)
# Multimodal: 11B e 90B parâmetros (texto + imagem)
# Ideal para: Aplicações móveis ou web com suporte a texto e imagem (ex: currículos com gráficos, certificados em PDF).
# Adequação: Útil se você precisa analisar tanto texto quanto elementos visuais em um currículo. Versões menores rodam localmente; as maiores, na nuvem.
# Limitações: As versões pequenas têm capacidade limitada para raciocínio profundo, como análise semântica avançada.

# Qwen2.5 (Para avaliação criteriosa, profunda e multilíngue)
# Tamanhos disponíveis:
# De 0.5B a 72B parâmetros (inclusive modelos com Mixture-of-Experts)
# Ideal para: Processamento avançado de linguagem, entendimento multilingue, análise semântica profunda de currículos complexos.
# Adequação: Melhor escolha para avaliar perfis profissionais com profundidade, especialmente em vários idiomas (ex: currículos em inglês, português, espanhol).
# Limitações: Requer mais recursos computacionais (ex: GPU com 24 GB de VRAM para os maiores).

# Apagar os modelos do conteiner do Ollama (se necessário).
# docker exec -it ollama /bin/bash
# rm -rf ~/.ollama/models/blobs/*